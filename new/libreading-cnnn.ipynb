{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# using the 39 phone set proposed in (Lee & Hon, 1989)\n# Table 3. Mapping from 61 classes to 39 classes, as proposed by Lee and Hon, (Lee & Hon,\n# 1989). The phones in the left column are folded into the labels of the right column. The\n# remaining phones are left intact.}\n!pip install trax\nimport trax\nimport trax.layers as tl\n\nimport random as rnd\nimport json\nimport numpy as np\nfrom trax.fastmath import numpy as fastnp\nimport os\n!apt-get update && apt-get install -y python3-opencv\n!pip install opencv-python\nimport cv2\nimport itertools","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef collect_video_files(data_type, resp, root_dir):\n    if resp == 'lipspeakers':\n        speakerDir = root_dir+'/lipspeakers'\n    elif resp == 'volunteers':\n        speakerDir = root_dir+'/volunteers'\n    else:\n        speakerDir = root_dir\n\n    fileName = f'{data_type}_dict.json'\n    data = open(speakerDir+os.sep+fileName).read()\n    data_dict = json.loads(data)\n\n    with open(f'{speakerDir}/data_phn.txt', 'r') as f:\n        lines = f.readlines()\n    video_list = [l.strip() for l in lines]\n\n    return data_dict, video_list, speakerDir\n\n\n\n\nvideo_path = os.path.join('/kaggle/input/volunt')\nspkr_type = 'volunteerss'\n\nlip_data_dict, lip_video_list, lip_data_dir = collect_video_files('data', 'volunteers',video_path)\n\nlip_video_list=list(set(lip_video_list))\nrnd.shuffle(lip_video_list)\ntrain_size=int(len(lip_video_list)*.9)\ntrain_list=lip_video_list[:train_size]\n\nval_list=lip_video_list[train_size:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_list),len(val_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phoneme_set_39_list = [\n    'iy', 'ih', 'eh', 'ae', 'ah', 'uw', 'uh', 'aa', 'ey', 'ay', 'oy', 'aw', 'ow',\n    'l', 'r', 'y', 'w', 'er', 'm', 'n', 'ng', 'ch', 'jh', 'dh', 'b', 'd', 'dx',\n    'g', 'p', 't', 'k', 'z', 'v', 'f', 'th', 's', 'sh', 'hh', 'sil'\n]\ndef get_vocabs(phoneme_set_39_list):\n    phonem_nunmber={'<pad>':0}\n    number_phonem={0:'<pad>'}\n    for phonem in phoneme_set_39_list:\n        phonem_nunmber[phonem]=len(phonem_nunmber)\n        number_phonem[len(number_phonem)]=phonem\n    return phonem_nunmber,number_phonem\n\nphonem_nunmber,number_phonem=get_vocabs(phoneme_set_39_list)\nlen(phonem_nunmber),len(number_phonem)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gemerte_date(data,data_labels,data_dir,phonem_nunmber):\n    data_size=len(data)\n    index=[*range(data_size)]\n    idx=0\n    while True:\n        if idx>=data_size:\n            idx=0\n        rnd.shuffle(index)\n        video=data[index[idx]]\n        \n        spk,vdo = video.split('_')[:2]\n        image_name = vdo.split('.')[0]+'_'\n\n        list_IDs_temp = sorted([image_file for image_file in list(data_labels) if image_name in image_file])\n        list_IDs_temp=sorted([image_file for image_file in list_IDs_temp if image_file in os.listdir(data_dir+'/'+spk)])\n        \n        idx+=1\n        X = np.zeros((len(list_IDs_temp),120,120,1))\n        Y = []\n        for i, ID in enumerate(list_IDs_temp):\n\n            im = cv2.imread(data_dir+'/'+spk+'/'+ID,0)\n            im=np.expand_dims(im,axis=-1)\n            if im.shape!=(120,120,1):\n                print(f'image shape is {im.shape}')\n            X[i] = im\n            Y.append(phonem_nunmber[data_labels[ID]])\n        Y=np.array(Y)\n        yield ((X,Y))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:02:19.495826Z","iopub.execute_input":"2023-05-08T11:02:19.496622Z","iopub.status.idle":"2023-05-08T11:02:19.508835Z","shell.execute_reply.started":"2023-05-08T11:02:19.496584Z","shell.execute_reply":"2023-05-08T11:02:19.507918Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"X,Y=next(gemerte_date(train_list,lip_data_dict,lip_data_dir,phonem_nunmber))\nX.shape,Y.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:02:19.785026Z","iopub.execute_input":"2023-05-08T11:02:19.785737Z","iopub.status.idle":"2023-05-08T11:02:20.356113Z","shell.execute_reply.started":"2023-05-08T11:02:19.785703Z","shell.execute_reply":"2023-05-08T11:02:20.355196Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"((60, 120, 120, 1), (60,))"},"metadata":{}}]},{"cell_type":"code","source":"boundaries =  [8,   16,  32, 64, 128, 256, 512]\nbatch_sizes = [512,256,128, 64, 32, 16,    8,   4]\n\n\n# Create the generators.\ntrain_batch_stream = trax.data.BucketByLength(\n    boundaries, batch_sizes,\n    length_keys=[0, 1]  # As before: count inputs and targets to length.\n)(gemerte_date(train_list,lip_data_dict,lip_data_dir,phonem_nunmber))\n\nval_batch_stream = trax.data.BucketByLength(\n    boundaries, batch_sizes,\n    length_keys=[0, 1]  # As before: count inputs and targets to length.\n)(gemerte_date(val_list,lip_data_dict,lip_data_dir,phonem_nunmber))\n\n\ntrain_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\nval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(val_batch_stream)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:02:20.357525Z","iopub.execute_input":"2023-05-08T11:02:20.357805Z","iopub.status.idle":"2023-05-08T11:02:20.365465Z","shell.execute_reply.started":"2023-05-08T11:02:20.357780Z","shell.execute_reply":"2023-05-08T11:02:20.364606Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"X,Y,M=next(train_batch_stream)\nX.shape,Y.shape,M.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:02:21.422152Z","iopub.execute_input":"2023-05-08T11:02:21.422571Z","iopub.status.idle":"2023-05-08T11:03:15.093795Z","shell.execute_reply.started":"2023-05-08T11:02:21.422540Z","shell.execute_reply":"2023-05-08T11:03:15.092564Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"((64, 64, 120, 120, 1), (64, 64), (64, 64))"},"metadata":{}}]},{"cell_type":"code","source":"#CNN\ndef flatten_time_step(x):\n    batch_size, time_steps, height, width, channels = x.shape\n    return np.reshape(x, (batch_size, time_steps, height * width * channels))\n\ndef create_CNN():\n    model = tl.Serial(\n        tl.Conv(64, (3, 3), padding='SAME'),\n        tl.BatchNorm(),\n        tl.Relu(),\n        tl.MaxPool((1,2,2),strides=(1,2,2)),\n\n        tl.Conv(64, (3, 3), padding='SAME'),\n        tl.BatchNorm(),\n        tl.Relu(),\n        tl.MaxPool((1,2,2),strides=(1,2,2)),\n        \n        tl.Conv(128, (3, 3), padding='SAME'),\n        tl.BatchNorm(),\n        tl.Relu(),\n        tl.MaxPool((1,2,2),strides=(1,2,2)),\n\n        tl.Conv(128, (3, 3), padding='SAME'),\n        tl.BatchNorm(),\n        tl.Relu(),\n        tl.MaxPool((1,2,2),strides=(1,2,2)),\n\n\n        \n        tl.Conv(256, (3, 3), padding='SAME'),\n        tl.BatchNorm(),\n        tl.Relu(),\n        tl.MaxPool((1,2,2),strides=(1,3,3)),\n        \n\n\n\n        tl.Fn('flatten_time_step',flatten_time_step)\n\n\n    )\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:16:02.926161Z","iopub.execute_input":"2023-05-08T11:16:02.927106Z","iopub.status.idle":"2023-05-08T11:16:02.939253Z","shell.execute_reply.started":"2023-05-08T11:16:02.927061Z","shell.execute_reply":"2023-05-08T11:16:02.938389Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"### UNQ_C1\n# GRADED FUNCTION\ndef input_encoder_fn(n_encoder_layers):\n    \"\"\" Input encoder runs on the input sentence and creates\n    activations that will be the keys and values for attention.\n    \n    Args:\n        input_vocab_size: int: vocab size of the input\n        d_model: int:  depth of embedding (n_units in the LSTM cell)\n        n_encoder_layers: int: number of LSTM layers in the encoder\n    Returns:\n        tl.Serial: The input encoder\n    \"\"\"\n    \n    # create a serial network\n    input_encoder = tl.Serial( \n\n        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n        [tl.LSTM(n_units=1024) for _ in range(n_encoder_layers)]\n        ### END CODE HERE ###\n    )\n\n    return input_encoder","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:16:03.789341Z","iopub.execute_input":"2023-05-08T11:16:03.790264Z","iopub.status.idle":"2023-05-08T11:16:03.795700Z","shell.execute_reply.started":"2023-05-08T11:16:03.790230Z","shell.execute_reply":"2023-05-08T11:16:03.794740Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# UNQ_C2\n# GRADED FUNCTION\ndef pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n    \"\"\" Pre-attention decoder runs on the targets and creates\n    activations that are used as queries in attention.\n    \n    Args:\n        mode: str: 'train' or 'eval'\n        target_vocab_size: int: vocab size of the target\n        d_model: int:  depth of embedding (n_units in the LSTM cell)\n    Returns:\n        tl.Serial: The pre-attention decoder\n    \"\"\"\n    \n    # create a serial network\n    pre_attention_decoder = tl.Serial(\n        \n        ### START CODE HERE ###\n        # shift right to insert start-of-sentence token and implement\n        # teacher forcing during training\n        tl.ShiftRight(),\n\n        # run an embedding layer to convert tokens to vectors\n        tl.Embedding(target_vocab_size,d_model),\n\n        # feed to an LSTM layer\n        tl.LSTM(d_model)\n        ### END CODE HERE ###\n    )\n    \n    return pre_attention_decoder","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:16:05.259910Z","iopub.execute_input":"2023-05-08T11:16:05.260892Z","iopub.status.idle":"2023-05-08T11:16:05.267070Z","shell.execute_reply.started":"2023-05-08T11:16:05.260851Z","shell.execute_reply":"2023-05-08T11:16:05.266027Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# UNQ_C3\n# GRADED FUNCTION\ndef prepare_attention_input(encoder_activations, decoder_activations, inputs):\n    \"\"\"Prepare queries, keys, values and mask for attention.\n    \n    Args:\n        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n        inputs fastnp.array(batch_size, padded_input_length): input tokens\n    \n    Returns:\n        queries, keys, values and mask for attention.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # set the keys and values to the encoder activations\n    keys = encoder_activations\n    values = encoder_activations\n\n    \n    # set the queries to the decoder activations\n    queries = decoder_activations\n    \n    # generate the mask to distinguish real tokens from padding\n    # hint: inputs is positive for real tokens and 0 where they are padding\n    mask =(np.sum(inputs,axis=(-1,-2,-3)))\n    mask=mask!=0\n    ### END CODE HERE ###\n    # add axes to the mask for attention heads and decoder length.\n    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n    \n    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n    # note: for this assignment, attention heads is set to 1.\n    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n  \n        \n    \n    return queries, keys, values, mask","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:16:06.600982Z","iopub.execute_input":"2023-05-08T11:16:06.601971Z","iopub.status.idle":"2023-05-08T11:16:06.609769Z","shell.execute_reply.started":"2023-05-08T11:16:06.601937Z","shell.execute_reply":"2023-05-08T11:16:06.608877Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# UNQ_C4\n# GRADED FUNCTION\ndef NMTAttn(\n            target_vocab_size=len(phonem_nunmber),\n            d_model=1024,\n            n_encoder_layers=6,\n            n_decoder_layers=6,\n            n_attention_heads=8,\n            attention_dropout=0.1,\n            mode='train'):\n    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n\n    The input to the model is a pair (input tokens, target tokens), e.g.,\n    an English sentence (tokenized) and its translation into German (tokenized).\n\n    Args:\n    input_vocab_size: int: vocab size of the input\n    target_vocab_size: int: vocab size of the target\n    d_model: int:  depth of embedding (n_units in the LSTM cell)\n    n_encoder_layers: int: number of LSTM layers in the encoder\n    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n    n_attention_heads: int: number of attention heads\n    attention_dropout: float, dropout for the attention layer\n    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n\n    Returns:\n    An LSTM sequence-to-sequence model with attention.\n    \"\"\"\n\n    ### START CODE HERE ###\n    \n    # Step 0: call the helper function to create layers for the input encoder\n    input_encoder = input_encoder_fn(n_encoder_layers)\n\n    # Step 0: call the helper function to create layers for the pre-attention decoder\n    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n    CNN=create_CNN()\n    # Step 1: create a serial network\n    model = tl.Serial( \n        \n      # Step 2: copy input tokens and target tokens as they will be needed later.\n      tl.Select([0,1,0,1]),\n        \n        CNN,\n        \n        \n      # Step 3: run input encoder on the input and pre-attention decoder the target.\n      tl.Parallel(input_encoder,pre_attention_decoder),\n        \n      # Step 4: prepare queries, keys, values and mask for attention.\n      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n        \n      # Step 5: run the AttentionQKV layer\n      # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n      \n      # Step 6: drop attention mask (i.e. index = None\n      tl.Select([0,2]),\n        \n      # Step 7: run the rest of_de the RNN decoder\n      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n        \n      # Step 8: prepare output by making it the right size\n      tl.Dense(target_vocab_size),   \n      # Step 9: Log-softmax for output\n      tl.LogSoftmax()\n    )\n    \n    ### END CODE HERE\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:16:10.742665Z","iopub.execute_input":"2023-05-08T11:16:10.743168Z","iopub.status.idle":"2023-05-08T11:16:10.754528Z","shell.execute_reply.started":"2023-05-08T11:16:10.743135Z","shell.execute_reply":"2023-05-08T11:16:10.753601Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# UNQ_C5\n# GRADED PART\nfrom trax.supervised import training\ndef train_task_function(train_batch_stream):\n    \"\"\"Returns a trax.training.TrainTask object.\n\n    Args:\n    train_batch_stream generator: labeled data generator\n\n    Returns:\n    A trax.training.TrainTask object.\n    \"\"\"\n\n    return training.TrainTask(\n\n        ### START CODE HERE\n\n        # use the train batch stream as labeled (\n        labeled_data= train_batch_stream,\n\n        # use the cross entropy loss\n        loss_layer= tl.CrossEntropyLoss(),\n\n        # use the Adam optimizer with learning rate of 0.01\n        optimizer= trax.optimizers.Adam(0.0001),\n           \n        lr_schedule=lr_schedule,\n        # use the `trax.lr.warmup_and_rsqrt_decay` as th.01e learning rate schedule\n        # have 1000 warmup steps with a max value of 0.01\n        \n\n        # have a checkpoint every 10 steps\n        n_steps_per_checkpoint= 10\n\n        ### END CODE HERE\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:16:12.385087Z","iopub.execute_input":"2023-05-08T11:16:12.385770Z","iopub.status.idle":"2023-05-08T11:16:12.393005Z","shell.execute_reply.started":"2023-05-08T11:16:12.385736Z","shell.execute_reply":"2023-05-08T11:16:12.392015Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train_task = train_task_function(train_batch_stream)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:16:13.404557Z","iopub.execute_input":"2023-05-08T11:16:13.405288Z","iopub.status.idle":"2023-05-08T11:17:10.482607Z","shell.execute_reply.started":"2023-05-08T11:16:13.405246Z","shell.execute_reply":"2023-05-08T11:17:10.481481Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#x,y,m=next(train_batch_stream)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:17:10.484456Z","iopub.execute_input":"2023-05-08T11:17:10.484742Z","iopub.status.idle":"2023-05-08T11:17:10.488427Z","shell.execute_reply.started":"2023-05-08T11:17:10.484717Z","shell.execute_reply":"2023-05-08T11:17:10.487584Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"eval_task = training.EvalTask(\n    \n    ## use the eval batch stream as labeled data\n    labeled_data=val_batch_stream,\n    \n    ## use the cross entropy loss and accuracy as metrics\n    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n    n_eval_batches=1\n    \n)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:17:14.592676Z","iopub.execute_input":"2023-05-08T11:17:14.593492Z","iopub.status.idle":"2023-05-08T11:18:22.294154Z","shell.execute_reply.started":"2023-05-08T11:17:14.593457Z","shell.execute_reply":"2023-05-08T11:18:22.292641Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# define the output directory\noutput_dir = 'output_dir/'\n\n\n# define the training loop\ntraining_loop = training.Loop(NMTAttn(mode='train'),\n                              train_task,\n                              eval_tasks=[eval_task],\n                              output_dir=output_dir)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:18:22.296377Z","iopub.execute_input":"2023-05-08T11:18:22.297044Z","iopub.status.idle":"2023-05-08T11:18:30.483938Z","shell.execute_reply.started":"2023-05-08T11:18:22.297010Z","shell.execute_reply":"2023-05-08T11:18:30.482633Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/jax/_src/xla_bridge.py:613: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/trax/layers/normalization.py:94: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n  n_batches = jnp.zeros((), dtype=jnp.int64)\n","output_type":"stream"},{"name":"stdout","text":"mask.shape= (64, 1, 64, 64)\nmask.shape= (64, 1, 64, 64)\nmask.shape= (64, 1, 64, 64)\n","output_type":"stream"}]},{"cell_type":"code","source":"n_step=int(len(train_list)/32)*100\nprint(n_step)\ntraining_loop.run(n_step)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:18:30.485283Z","iopub.execute_input":"2023-05-08T11:18:30.485592Z","iopub.status.idle":"2023-05-08T11:42:34.492787Z","shell.execute_reply.started":"2023-05-08T11:18:30.485565Z","shell.execute_reply":"2023-05-08T11:42:34.491351Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"1460\nmask.shape= (8, 1, 64, 64)\n\nStep      1: Total number of trainable weights: 30050792\nStep      1: Ran 1 train steps in 111.15 secs\nStep      1: train CrossEntropyLoss |  3.82385254\nmask.shape= (8, 1, 64, 64)\nStep      1: eval  CrossEntropyLoss |  3.78065634\nStep      1: eval          Accuracy |  0.02950941\nmask.shape= (16, 1, 32, 32)\n\nStep      2: Ran 1 train steps in 47.60 secs\nStep      2: train CrossEntropyLoss |  3.78216553\nmask.shape= (16, 1, 32, 32)\nStep      2: eval  CrossEntropyLoss |  3.68455267\nStep      2: eval          Accuracy |  0.07039397\n\nStep      3: Ran 1 train steps in 84.70 secs\nStep      3: train CrossEntropyLoss |  3.62646556\nStep      3: eval  CrossEntropyLoss |  3.50699186\nStep      3: eval          Accuracy |  0.11127327\n\nStep      4: Ran 1 train steps in 79.79 secs\nStep      4: train CrossEntropyLoss |  3.51234603\nStep      4: eval  CrossEntropyLoss |  3.44409657\nStep      4: eval          Accuracy |  0.11478454\n\nStep      5: Ran 1 train steps in 64.67 secs\nStep      5: train CrossEntropyLoss |  3.54024887\nStep      5: eval  CrossEntropyLoss |  3.37290072\nStep      5: eval          Accuracy |  0.11947387\n\nStep      6: Ran 1 train steps in 27.24 secs\nStep      6: train CrossEntropyLoss |  3.39465761\nStep      6: eval  CrossEntropyLoss |  3.45102882\nStep      6: eval          Accuracy |  0.07836358\n\nStep      7: Ran 1 train steps in 82.16 secs\nStep      7: train CrossEntropyLoss |  3.36324263\nStep      7: eval  CrossEntropyLoss |  3.40501928\nStep      7: eval          Accuracy |  0.06366514\n\nStep      8: Ran 1 train steps in 76.13 secs\nStep      8: train CrossEntropyLoss |  3.38859272\nStep      8: eval  CrossEntropyLoss |  3.39366388\nStep      8: eval          Accuracy |  0.11714392\n\nStep      9: Ran 1 train steps in 53.10 secs\nStep      9: train CrossEntropyLoss |  3.44493580\nStep      9: eval  CrossEntropyLoss |  3.46700335\nStep      9: eval          Accuracy |  0.13605240\n\nStep     10: Ran 1 train steps in 37.07 secs\nStep     10: train CrossEntropyLoss |  3.35518074\nStep     10: eval  CrossEntropyLoss |  3.37420344\nStep     10: eval          Accuracy |  0.11443850\n\nStep     11: Ran 1 train steps in 68.06 secs\nStep     11: train CrossEntropyLoss |  3.38414431\nStep     11: eval  CrossEntropyLoss |  3.38008261\nStep     11: eval          Accuracy |  0.11312058\n\nStep     12: Ran 1 train steps in 75.60 secs\nStep     12: train CrossEntropyLoss |  3.34876299\nStep     12: eval  CrossEntropyLoss |  3.36051702\nStep     12: eval          Accuracy |  0.11441728\n\nStep     13: Ran 1 train steps in 52.98 secs\nStep     13: train CrossEntropyLoss |  3.41606379\nStep     13: eval  CrossEntropyLoss |  3.40281510\nStep     13: eval          Accuracy |  0.12489307\n\nStep     14: Ran 1 train steps in 43.04 secs\nStep     14: train CrossEntropyLoss |  3.35061550\nStep     14: eval  CrossEntropyLoss |  3.35132551\nStep     14: eval          Accuracy |  0.10965689\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(n_step)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_step\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:435\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_changed:\n\u001b[1;32m    433\u001b[0m   loss_acc, step_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 435\u001b[0m loss, optimizer_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_changed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# optimizer_metrics and loss are replicated on self.n_devices, a few\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# metrics are replicated (ex: gradients_l2, weights_l2) - i.e. they are\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# the same across devices, whereas some (ex: loss) aren't because they\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# implies the loss here is averaged from this hosts' devices and not\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# across all hosts.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m optimizer_metrics, loss \u001b[38;5;241m=\u001b[39m fastmath\u001b[38;5;241m.\u001b[39mnested_map(\n\u001b[1;32m    447\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(tl\u001b[38;5;241m.\u001b[39mmean_or_pmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_devices),\n\u001b[1;32m    448\u001b[0m     (optimizer_metrics, loss))\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:625\u001b[0m, in \u001b[0;36mLoop._run_one_step\u001b[0;34m(self, task_index, task_changed)\u001b[0m\n\u001b[1;32m    622\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_step_begin(step)\n\u001b[1;32m    624\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks[task_index]\u001b[38;5;241m.\u001b[39mlearning_rate(step)\n\u001b[0;32m--> 625\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m rng \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_rng()\n\u001b[1;32m    627\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_per_task[task_index]\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/trax/supervised/training.py:1101\u001b[0m, in \u001b[0;36mTrainTask.next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns one batch of labeled data: a tuple of input(s) plus label.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_labeled_data\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/trax/data/inputs.py:864\u001b[0m, in \u001b[0;36madd_loss_weights\u001b[0;34m(generator, id_to_mask)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;129m@debug_data_pipeline\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_pipeline\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_loss_weights\u001b[39m(generator, id_to_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    846\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Add weights to inputs without weights and masks by id if requested.\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m  The generator stream is augmented in the following way:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Examples from the augmented stream.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(example) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(example) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    866\u001b[0m       \u001b[38;5;28;01massert\u001b[39;00m id_to_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot automatically mask this stream.\u001b[39m\u001b[38;5;124m'\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/trax/data/inputs.py:828\u001b[0m, in \u001b[0;36mbucket_by_length\u001b[0;34m(generator, length_fn, boundaries, batch_sizes, strict_pad_on_len)\u001b[0m\n\u001b[1;32m    826\u001b[0m buckets \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_sizes))]\n\u001b[1;32m    827\u001b[0m boundaries \u001b[38;5;241m=\u001b[39m boundaries \u001b[38;5;241m+\u001b[39m [math\u001b[38;5;241m.\u001b[39minf]  \u001b[38;5;66;03m# Max boundary is unlimited.\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m    829\u001b[0m   length \u001b[38;5;241m=\u001b[39m length_fn(example)\n\u001b[1;32m    830\u001b[0m   \u001b[38;5;66;03m# `bucket_idx` will always be < len(boundaries), since boundaries is right\u001b[39;00m\n\u001b[1;32m    831\u001b[0m   \u001b[38;5;66;03m# padded by `math.inf`.\u001b[39;00m\n","Cell \u001b[0;32mIn[32], line 15\u001b[0m, in \u001b[0;36mgemerte_date\u001b[0;34m(data, data_labels, data_dir, phonem_nunmber)\u001b[0m\n\u001b[1;32m     12\u001b[0m image_name \u001b[38;5;241m=\u001b[39m vdo\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m list_IDs_temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(data_labels) \u001b[38;5;28;01mif\u001b[39;00m image_name \u001b[38;5;129;01min\u001b[39;00m image_file])\n\u001b[0;32m---> 15\u001b[0m list_IDs_temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m list_IDs_temp \u001b[38;5;28;01mif\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mspk)])\n\u001b[1;32m     17\u001b[0m idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(list_IDs_temp),\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n","Cell \u001b[0;32mIn[32], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m image_name \u001b[38;5;241m=\u001b[39m vdo\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m list_IDs_temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(data_labels) \u001b[38;5;28;01mif\u001b[39;00m image_name \u001b[38;5;129;01min\u001b[39;00m image_file])\n\u001b[0;32m---> 15\u001b[0m list_IDs_temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m list_IDs_temp \u001b[38;5;28;01mif\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mspk\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     17\u001b[0m idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(list_IDs_temp),\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working'\nremove_folder_contents(folder_path)\nos.rmdir(r'/kaggle/working/model')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T20:13:41.353350Z","iopub.execute_input":"2023-05-05T20:13:41.354377Z","iopub.status.idle":"2023-05-05T20:13:41.698438Z","shell.execute_reply.started":"2023-05-05T20:13:41.354329Z","shell.execute_reply":"2023-05-05T20:13:41.696904Z"},"trusted":true},"execution_count":115,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[115], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m remove_folder_contents(folder_path)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/model'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/model'","output_type":"error"}]},{"cell_type":"code","source":"x,y,m=next(val_batch_stream)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:25:11.912842Z","iopub.execute_input":"2023-05-05T19:25:11.913640Z","iopub.status.idle":"2023-05-05T19:25:34.056940Z","shell.execute_reply.started":"2023-05-05T19:25:11.913605Z","shell.execute_reply":"2023-05-05T19:25:34.055388Z"},"trusted":true},"execution_count":49,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x,y,m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_batch_stream\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/trax/data/inputs.py:864\u001b[0m, in \u001b[0;36madd_loss_weights\u001b[0;34m(generator, id_to_mask)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;129m@debug_data_pipeline\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_pipeline\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_loss_weights\u001b[39m(generator, id_to_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    846\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Add weights to inputs without weights and masks by id if requested.\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m  The generator stream is augmented in the following way:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Examples from the augmented stream.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(example) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(example) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    866\u001b[0m       \u001b[38;5;28;01massert\u001b[39;00m id_to_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot automatically mask this stream.\u001b[39m\u001b[38;5;124m'\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/trax/data/inputs.py:828\u001b[0m, in \u001b[0;36mbucket_by_length\u001b[0;34m(generator, length_fn, boundaries, batch_sizes, strict_pad_on_len)\u001b[0m\n\u001b[1;32m    826\u001b[0m buckets \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_sizes))]\n\u001b[1;32m    827\u001b[0m boundaries \u001b[38;5;241m=\u001b[39m boundaries \u001b[38;5;241m+\u001b[39m [math\u001b[38;5;241m.\u001b[39minf]  \u001b[38;5;66;03m# Max boundary is unlimited.\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m    829\u001b[0m   length \u001b[38;5;241m=\u001b[39m length_fn(example)\n\u001b[1;32m    830\u001b[0m   \u001b[38;5;66;03m# `bucket_idx` will always be < len(boundaries), since boundaries is right\u001b[39;00m\n\u001b[1;32m    831\u001b[0m   \u001b[38;5;66;03m# padded by `math.inf`.\u001b[39;00m\n","Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mgemerte_date\u001b[0;34m(data, data_labels, data_dir, phonem_nunmber, shuffle)\u001b[0m\n\u001b[1;32m     14\u001b[0m image_name \u001b[38;5;241m=\u001b[39m vdo\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m list_IDs_temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(data_labels) \u001b[38;5;28;01mif\u001b[39;00m image_name \u001b[38;5;129;01min\u001b[39;00m image_file])\n\u001b[0;32m---> 17\u001b[0m list_IDs_temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m list_IDs_temp \u001b[38;5;28;01mif\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mspk)])\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(list_IDs_temp),\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     22\u001b[0m Y \u001b[38;5;241m=\u001b[39m []\n","Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m image_name \u001b[38;5;241m=\u001b[39m vdo\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m list_IDs_temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(data_labels) \u001b[38;5;28;01mif\u001b[39;00m image_name \u001b[38;5;129;01min\u001b[39;00m image_file])\n\u001b[0;32m---> 17\u001b[0m list_IDs_temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m([image_file \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m list_IDs_temp \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimage_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mspk\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(list_IDs_temp),\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     22\u001b[0m Y \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import itertools","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:24:32.434237Z","iopub.execute_input":"2023-05-05T19:24:32.435033Z","iopub.status.idle":"2023-05-05T19:24:32.439669Z","shell.execute_reply.started":"2023-05-05T19:24:32.434998Z","shell.execute_reply":"2023-05-05T19:24:32.438319Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"a=itertools.cycle(val_batch_stream)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:24:53.460325Z","iopub.execute_input":"2023-05-05T19:24:53.461454Z","iopub.status.idle":"2023-05-05T19:24:53.466107Z","shell.execute_reply.started":"2023-05-05T19:24:53.461413Z","shell.execute_reply":"2023-05-05T19:24:53.464878Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"next(train_batch_stream)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T20:15:02.754610Z","iopub.execute_input":"2023-05-05T20:15:02.755761Z","iopub.status.idle":"2023-05-05T20:15:02.790091Z","shell.execute_reply.started":"2023-05-05T20:15:02.755718Z","shell.execute_reply":"2023-05-05T20:15:02.788615Z"},"trusted":true},"execution_count":121,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch_stream\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mStopIteration\u001b[0m: "],"ename":"StopIteration","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}